{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gothchico/handwritten-digit-recognition/blob/master/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxKa5r7Ll7Dg",
        "colab_type": "text"
      },
      "source": [
        "<h1>Machine Learning - Homework IV </h1>\n",
        "\n",
        "### Sarah Seifi, ist197658, sarah.seifi@tecnico.ulisboa.pt\n",
        "### Mahima Raut, ist197569, cs17b112@smail.iitm.ac.in\n",
        "\n",
        "\n",
        "In this notebook, different neural network architectures are used to achieve the highest performance on the test set of the famous [MNIST](http://yann.lecun.com/exdb/mnist/) data set. \n",
        "\n",
        "\n",
        "1.   First, the use of Feed-Forward and Convolutional Neural Networks are explored.\n",
        "2.   Secondly, the networks are explored using different numbers of hidden layers.\n",
        "3.   As a last step, the impact of different regularization methods is assessed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icnrt-Vg3lXw",
        "colab_type": "text"
      },
      "source": [
        "# **Importing Libraries**\n",
        "\n",
        "\n",
        "* [tensorflow](https://www.tensorflow.org/): the neural network library\n",
        "* [tensorflow_datasets](https://www.tensorflow.org/datasets): provides the datasets that we will use\n",
        "* [numpy](https://numpy.org/): we will use it to store the data in array format for visualization\n",
        "* [sklearn](https://scikit-learn.org/): provides a [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) implementation that we will use for visualization\n",
        "* [matplotlib](https://matplotlib.org/): plotting library for visualization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6VTCny05XSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import sklearn.decomposition\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbP1om07O3y_",
        "colab_type": "text"
      },
      "source": [
        "# Load the data\n",
        "\n",
        "The MNIST data set is widely used for introducing image classification problems. It has following characteristics: \n",
        "\n",
        "*   70k examples of handwritten digits\n",
        "*   Image size: 28x28\n",
        "*   1 channel\n",
        "*   10 classes: [0-9]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG9yA5cTPCIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_data, mnist_info = tfds.load('mnist', with_info=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uflD4f_cPNus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(mnist_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJpojr7xPQZY",
        "colab_type": "text"
      },
      "source": [
        "Now, the data set is converted into NumPy arrays:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlKEWc4NUdKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_train_x = np.asarray([instance['image']/255 for instance in tfds.as_numpy(mnist_data['train'])])\n",
        "mnist_train_y = np.asarray([instance['label'] for instance in tfds.as_numpy(mnist_data['train'])])\n",
        "\n",
        "mnist_test_x = np.asarray([instance['image']/255 for instance in tfds.as_numpy(mnist_data['test'])])\n",
        "mnist_test_y = np.asarray([instance['label'] for instance in tfds.as_numpy(mnist_data['test'])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_FsBBOWPS90",
        "colab_type": "text"
      },
      "source": [
        "# Models\n",
        "\n",
        "Here, some networks are created and trained to approach the problem posed by the MNIST dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJkvr6WMPWnC",
        "colab_type": "text"
      },
      "source": [
        "## Feed-Forward Neural Network\n",
        "\n",
        "Feed-Forward Neural Networks (FFNN) or also known multilayer perceptrons are the foundation of most deep learning models. Networks like CNNs and RNNs are just some special cases of FFNNs. These networks are mostly used for supervised machine learning tasks where we already know the target function.\n",
        "\n",
        "The main goal of a FFNN is to approximate some function f*. For example, a regression function y = f *(x) maps an input x to a value y. A FFNN defines a mapping y = f (x; θ) and learns the value of the parameters θ that result in the best function approximation.\n",
        "\n",
        "The reason these networks are called feed-forward is that the flow of information takes place in the forward direction.In this, if we add feedback from the last hidden layer to the first hidden layer it would represent a recurrent neural network.\n",
        "\n",
        "We need FFNN over linear machine learning models, because linear models are limited to only linear functions whereas neural networks aren’t.The hidden layers are used to increase the non-linearity and change the representation of the data for better generalization over the function.\n",
        "\n",
        "---\n",
        "\n",
        "As a first step, the FFNN is discussed. Before passing the images to the Dense layer, they are flattened:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPXvD6ZwPZ7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_baseline_model = tf.keras.Sequential(name='mnist_baseline')\n",
        "mnist_baseline_model.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n",
        "mnist_baseline_model.add(tf.keras.layers.Flatten(name='flatten'))\n",
        "mnist_baseline_model.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n",
        "mnist_baseline_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "mnist_baseline_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTqtYd0DPdgi",
        "colab_type": "text"
      },
      "source": [
        "The model can now be trained using part of the training set for validation, in order to control when to stop the training phase:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4lOYGYEPi1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_baseline_best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n",
        "\n",
        "mnist_baseline_model_train = mnist_baseline_model.fit(mnist_train_x, mnist_train_y, validation_split=0.2, callbacks=[earlystop,checkpoint], epochs=10000, batch_size=256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GYduFeVPl5G",
        "colab_type": "text"
      },
      "source": [
        "The evaluation of the evolvement of the performance on the training and validation data is done, by plotting the results:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-3M-CycPoBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n",
        "\n",
        "loss_ax.set_title('Loss')\n",
        "loss_ax.plot(mnist_baseline_model_train.history['loss'], '-r', label='Train')\n",
        "loss_ax.plot(mnist_baseline_model_train.history['val_loss'], '-g', label='Validation')\n",
        "\n",
        "acc_ax.set_title('Accuracy')\n",
        "acc_ax.plot(mnist_baseline_model_train.history['accuracy'], '-r', label='Train')\n",
        "acc_ax.plot(mnist_baseline_model_train.history['val_accuracy'], '-g', label='Validation')\n",
        "\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo-SQJ79Pqy2",
        "colab_type": "text"
      },
      "source": [
        "As a last step, the best model for the validation data is loaded and evaluated on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sZ9a5hVPu3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_baseline_model.load_weights('mnist_baseline_best.h5')\n",
        "loss, acc = mnist_baseline_model.evaluate(mnist_test_x, mnist_test_y)\n",
        "print('Accuracy: {}'.format(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13tVOFWonpZa",
        "colab_type": "text"
      },
      "source": [
        "## Convolutional Neural Networks\n",
        "\n",
        "To create our CNN, instead of feeding the flatenned output directly to the output layer, we will first pass it through a convolutional layer followed by a max pooling operation. Since, we are dealing with 2D data, we will use the [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) and [MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D) layers. \n",
        "\n",
        "In the convolutional layer, the *filters* parameter defines the number of kernels or filters used in the layer. The *kernel_size* parameter defines the size of the kernels. If only one number is provided, the kernel is assumed to be square. The stride values default to one, but can be changed using the *strides* parameter. Also, we can use same padding, by setting the *padding* parameter to 'same'.\n",
        "\n",
        "For the pooling operation, we define the size of the pooling window using the *pool_size* parameter. Similarly to the *kernel_size* parameter of the convolutional layer, if only one number is provided, the window is assumed to be square. Additionally, the *strides* parameter defaults to the size of the pooling window. That is, there is no overlap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsMkdg9bDvnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_conv_model = tf.keras.Sequential(name='mnist_cnn')\n",
        "mnist_conv_model.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n",
        "mnist_conv_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=4, activation='relu', padding='same', name='convolution'))\n",
        "mnist_conv_model.add(tf.keras.layers.MaxPool2D(pool_size=2, name='pooling'))\n",
        "mnist_conv_model.add(tf.keras.layers.Flatten(name='flatten'))\n",
        "mnist_conv_model.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n",
        "mnist_conv_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "mnist_conv_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVRHbY2xhb_V",
        "colab_type": "text"
      },
      "source": [
        "The model is trained using the prior approach:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bdf9pBFxTJiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_conv_best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n",
        "\n",
        "mnist_conv_model_train = mnist_conv_model.fit(mnist_train_x, mnist_train_y, validation_split=0.2, callbacks=[earlystop,checkpoint], epochs=10000, batch_size=256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGQ_i0n6hgui",
        "colab_type": "text"
      },
      "source": [
        "Again, the outcome is visualized:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gHkuL3GTKOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n",
        "\n",
        "loss_ax.set_title('Loss')\n",
        "loss_ax.plot(mnist_conv_model_train.history['loss'], '-r', label='Train')\n",
        "loss_ax.plot(mnist_conv_model_train.history['val_loss'], '-g', label='Validation')\n",
        "\n",
        "acc_ax.set_title('Accuracy')\n",
        "acc_ax.plot(mnist_conv_model_train.history['accuracy'], '-r', label='Train')\n",
        "acc_ax.plot(mnist_conv_model_train.history['val_accuracy'], '-g', label='Validation')\n",
        "\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpVI9gzahlHI",
        "colab_type": "text"
      },
      "source": [
        "As a last step, the model is evaluated on the test set, and it is verified that the performance is higher than without the convolutional layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR0GjqibTMO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_conv_model.load_weights('mnist_conv_best.h5')\n",
        "loss, acc = mnist_conv_model.evaluate(mnist_test_x, mnist_test_y)\n",
        "print('Accuracy: {}'.format(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePWWEAxFQmgV",
        "colab_type": "text"
      },
      "source": [
        "#Explore networks with a different number of hidden layers\n",
        "\n",
        "As a next step, network models with varying hidden layers are explored. For this, a FFNN with one and two hidden layers, respectively, are implemented. Afterwards, a CNN with two hidden layers is illustrated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWyFtml1qznd",
        "colab_type": "text"
      },
      "source": [
        "## Feed-Forward Network with One Hidden Layer\n",
        "\n",
        "A FFNN with one hidden layer is set up:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqQKdTNH_AjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_ffnn_1_model = tf.keras.Sequential(name='mnist_ffnn_1')\n",
        "mnist_ffnn_1_model.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n",
        "mnist_ffnn_1_model.add(tf.keras.layers.Flatten(name='flatten'))\n",
        "mnist_ffnn_1_model.add(tf.keras.layers.Dense(256,name='hidden_layer1',activation='tanh'))\n",
        "mnist_ffnn_1_model.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n",
        "mnist_ffnn_1_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "mnist_ffnn_1_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1RV5c5UDn3u",
        "colab_type": "text"
      },
      "source": [
        "Now we can train the model. We will still use part of the training set for validation, in order to control when to stop the training phase:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOvEBs1zDhT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_ffnn_1_best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n",
        "\n",
        "mnist_ffnn_1_model_train = mnist_ffnn_1_model.fit(mnist_train_x, mnist_train_y, validation_split=0.2, callbacks=[earlystop,checkpoint], epochs=10000, batch_size=256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlHPYsRSEb7v",
        "colab_type": "text"
      },
      "source": [
        "Let's see how the performance evolved on the training and validation data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMBUhHEjB7yJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n",
        "\n",
        "loss_ax.set_title('Loss')\n",
        "loss_ax.plot(mnist_ffnn_1_model_train.history['loss'], '-r', label='Train')\n",
        "loss_ax.plot(mnist_ffnn_1_model_train.history['val_loss'], '-g', label='Validation')\n",
        "\n",
        "acc_ax.set_title('Accuracy')\n",
        "acc_ax.plot(mnist_ffnn_1_model_train.history['accuracy'], '-r', label='Train')\n",
        "acc_ax.plot(mnist_ffnn_1_model_train.history['val_accuracy'], '-g', label='Validation')\n",
        "\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh6Rl6NFEtrG",
        "colab_type": "text"
      },
      "source": [
        "Now let's load the best model for the validation data and evaluate it on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmWBNSEOE7hB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "16ce2c12-8c83-4314-b0c4-f5423dd7ce86"
      },
      "source": [
        "mnist_ffnn_1_model.load_weights('mnist_ffnn_1_best.h5')\n",
        "loss, acc = mnist_ffnn_1_model.evaluate(mnist_test_x, mnist_test_y)\n",
        "print('Accuracy: {}'.format(acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0926 - accuracy: 0.9730\n",
            "Accuracy: 0.9729999899864197\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXzuWItlsA3Q",
        "colab_type": "text"
      },
      "source": [
        "## Feed-Forward Network with Two Hidden Layers\n",
        "\n",
        "Now, a FFNN with two hidden layers is implemented:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l38OX7gsATi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_ffnn_2_model = tf.keras.Sequential(name='mnist_ffnn_2')\n",
        "mnist_ffnn_2_model.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n",
        "mnist_ffnn_2_model.add(tf.keras.layers.Flatten(name='flatten'))\n",
        "mnist_ffnn_2_model.add(tf.keras.layers.Dense(256,name='hidden_layer1',activation='tanh'))\n",
        "mnist_ffnn_2_model.add(tf.keras.layers.Dense(256,name='hidden_layer2',activation='tanh'))\n",
        "mnist_ffnn_2_model.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n",
        "mnist_ffnn_2_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "mnist_ffnn_2_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJpqxU7HimWn",
        "colab_type": "text"
      },
      "source": [
        "Again, we use callbacks for stopping the training phase and finally train the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxyoJZVJsFTf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_ffnn_2_best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n",
        "\n",
        "mnist_ffnn_2_model_train = mnist_ffnn_2_model.fit(mnist_train_x, mnist_train_y, validation_split=0.2, callbacks=[earlystop,checkpoint], epochs=10000, batch_size=256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-jFAJzniqNd",
        "colab_type": "text"
      },
      "source": [
        "Visualizing the evolution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvkvPAAZsJD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n",
        "\n",
        "loss_ax.set_title('Loss')\n",
        "loss_ax.plot(mnist_ffnn_2_model_train.history['loss'], '-r', label='Train')\n",
        "loss_ax.plot(mnist_ffnn_2_model_train.history['val_loss'], '-g', label='Validation')\n",
        "\n",
        "acc_ax.set_title('Accuracy')\n",
        "acc_ax.plot(mnist_ffnn_2_model_train.history['accuracy'], '-r', label='Train')\n",
        "acc_ax.plot(mnist_ffnn_2_model_train.history['val_accuracy'], '-g', label='Validation')\n",
        "\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjxjL3zQivrE",
        "colab_type": "text"
      },
      "source": [
        "Although there is significant improvement in the accuracy when 1 hidden layer is introduced in contrast with the baseline model , we do not observe substantial improvement when hidden layers are increased from 1 to 2 in an FFNN. It does take double the time for every epoch approximately on an average, when 2 hidden layers exist instead of 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zT3_d2dsMmG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_baseline_model.load_weights('mnist_baseline_best.h5')\n",
        "loss, acc = mnist_baseline_model.evaluate(mnist_test_x, mnist_test_y)\n",
        "print('FFNN with 0 hidden layers; Accuracy: {}'.format(acc))\n",
        "\n",
        "mnist_ffnn_1_model.load_weights('mnist_ffnn_1_best.h5')\n",
        "loss, acc = mnist_ffnn_1_model.evaluate(mnist_test_x, mnist_test_y)\n",
        "print('FFNN with 1 hidden layer; Accuracy: {}'.format(acc))\n",
        "\n",
        "mnist_ffnn_2_model.load_weights('mnist_ffnn_2_best.h5')\n",
        "loss, acc = mnist_ffnn_2_model.evaluate(mnist_test_x, mnist_test_y)\n",
        "print('FFNN with 2 hidden layers; Accuracy: {}'.format(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojpZD9d5jGmJ",
        "colab_type": "text"
      },
      "source": [
        "| Number of Hidden Layers | Result |\n",
        "\n",
        " 0 - Only capable of representing linear separable functions or decisions.\n",
        "\n",
        " 1 - Can approximate any function that contains a continuous mapping\n",
        "from one finite space to another.\n",
        "\n",
        " 2+ - Can represent an arbitrary decision boundary to arbitrary accuracy\n",
        "with rational activation functions and can approximate any smooth\n",
        "mapping to any accuracy. Might lead to overfitting in some cases.\n",
        "\n",
        "In short, FFNN with two hidden layers can represent functions with any kind of shape. There is no reason to use FFNN with any more than two hidden layers. In fact, for many practical problems, there is no reason to use any more than one hidden layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5FgjjkxphfS",
        "colab_type": "text"
      },
      "source": [
        "## CNN with Two Hidden Layers \n",
        "\n",
        "For this CNN two hidden layers were added:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns8O3tIMpg99",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_cnn_2_model = tf.keras.Sequential(name='mnist_cnn_2')\n",
        "mnist_cnn_2_model.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n",
        "mnist_cnn_2_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=4, activation='relu', padding='same', name='convolution1'))\n",
        "mnist_cnn_2_model.add(tf.keras.layers.MaxPool2D(pool_size=2, name='pooling1'))\n",
        "mnist_cnn_2_model.add(tf.keras.layers.Conv2D(filters=25, kernel_size=3, activation='relu', padding='same', name='convolution2'))\n",
        "mnist_cnn_2_model.add(tf.keras.layers.MaxPool2D(pool_size=3, name='pooling2'))\n",
        "mnist_cnn_2_model.add(tf.keras.layers.Flatten(name='flatten'))\n",
        "mnist_cnn_2_model.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n",
        "mnist_cnn_2_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "mnist_cnn_2_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6id0QVJwjQdM",
        "colab_type": "text"
      },
      "source": [
        "The model is trained using the prior approach:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKe0Nhjcppg0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_cnn_2_best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n",
        "\n",
        "mnist_cnn_2_model_train = mnist_cnn_2_model.fit(mnist_train_x, mnist_train_y, validation_split=0.2, callbacks=[earlystop,checkpoint], epochs=10000, batch_size=256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NgvSCgHjV0k",
        "colab_type": "text"
      },
      "source": [
        "Again, the outcome is visualized:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-rbQuyupvcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n",
        "\n",
        "loss_ax.set_title('Loss')\n",
        "loss_ax.plot(mnist_cnn_2_model_train.history['loss'], '-r', label='Train')\n",
        "loss_ax.plot(mnist_cnn_2_model_train.history['val_loss'], '-g', label='Validation')\n",
        "\n",
        "acc_ax.set_title('Accuracy')\n",
        "acc_ax.plot(mnist_cnn_2_model_train.history['accuracy'], '-r', label='Train')\n",
        "acc_ax.plot(mnist_cnn_2_model_train.history['val_accuracy'], '-g', label='Validation')\n",
        "\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjaM9qc0jgST",
        "colab_type": "text"
      },
      "source": [
        "We can finally use this model on our test set and observe the results in comparison with CNN with 1 convoluted layer and with the baseline model (no convoluted layer) :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRB5F6VZpwku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_baseline_model.load_weights('mnist_baseline_best.h5')\n",
        "loss, acc = mnist_baseline_model.evaluate(mnist_test_x, mnist_test_y)\n",
        "print('Accuracy: {}'.format(acc))\n",
        "\n",
        "mnist_conv_model.load_weights('mnist_conv_best.h5')\n",
        "loss, acc = mnist_conv_model.evaluate(mnist_test_x, mnist_test_y)\n",
        "print('Accuracy: {}'.format(acc))\n",
        "\n",
        "mnist_cnn_2_model.load_weights('mnist_cnn_2_best.h5')\n",
        "loss, acc = mnist_cnn_2_model.evaluate(mnist_test_x, mnist_test_y)\n",
        "print('Accuracy: {}'.format(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGMk2JEGRd0u",
        "colab_type": "text"
      },
      "source": [
        "# Regularization Methods: General L2 Regularization and Dropout\n",
        "\n",
        "When adding layers to the network, we can also include regularization in those layers using three different parameters: kernel_regularizer, bias_regularizer, and activity_regularizer. The first applies regularization to the weights of the layer, the second to its bias, and the last to its output. Keras also has implementations of multiple regularizers. \n",
        "\n",
        "We take the best fitting models each from both the FFNNs and CNNs and apply L2 and dropout regularitaztion, respectively, to see if there's any improvement in the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXrouS_5RuF_",
        "colab_type": "text"
      },
      "source": [
        "## L2 Regularization on FFNN\n",
        "\n",
        "Taking FFNN with two hidden layers and applying L2 regularization to the weights of the hidden layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SicuT8qBSBh7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_ffnn_2_l2_model = tf.keras.Sequential(name='mnist_ffnn_2_l2')\n",
        "mnist_ffnn_2_l2_model.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n",
        "mnist_ffnn_2_l2_model.add(tf.keras.layers.Flatten(name='flatten'))\n",
        "mnist_ffnn_2_l2_model.add(tf.keras.layers.Dense(256,name='hidden_layer1',activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
        "mnist_ffnn_2_l2_model.add(tf.keras.layers.Dense(256,name='hidden_layer2',activation='tanh',kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
        "mnist_ffnn_2_l2_model.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n",
        "mnist_ffnn_2_l2_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "mnist_ffnn_2_l2_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iIO89JBjqXy",
        "colab_type": "text"
      },
      "source": [
        "Callbacks and Training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUheQP4_SHfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_ffnn_2_l2_best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n",
        "\n",
        "mnist_ffnn_2_l2_model_train = mnist_ffnn_2_l2_model.fit(mnist_train_x, mnist_train_y, validation_split=0.2, callbacks=[earlystop,checkpoint], epochs=10000, batch_size=256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT--EHR7jt6i",
        "colab_type": "text"
      },
      "source": [
        "Plotting accuracy and loss error of Training and Validation set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v94RZrv9SLjG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n",
        "\n",
        "loss_ax.set_title('Loss')\n",
        "loss_ax.plot(mnist_ffnn_2_l2_model_train.history['loss'], '-r', label='Train')\n",
        "loss_ax.plot(mnist_ffnn_2_l2_model_train.history['val_loss'], '-g', label='Validation')\n",
        "\n",
        "acc_ax.set_title('Accuracy')\n",
        "acc_ax.plot(mnist_ffnn_2_l2_model_train.history['accuracy'], '-r', label='Train')\n",
        "acc_ax.plot(mnist_ffnn_2_l2_model_train.history['val_accuracy'], '-g', label='Validation')\n",
        "\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_3KfXksj0sT",
        "colab_type": "text"
      },
      "source": [
        "We can see that the model converges faster. Also, by plotting the evolution, we can see that there are fewer oscillations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TggB316SRmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_baseline_model.load_weights('mnist_baseline_best.h5')\n",
        "loss, acc = mnist_baseline_model.evaluate(mnist_test_x, mnist_test_y)\n",
        "print('FFNN with 0 hidden layers; Accuracy: {}'.format(acc))\n",
        "\n",
        "mnist_ffnn_1_model.load_weights('mnist_ffnn_1_best.h5')\n",
        "loss, acc = mnist_ffnn_1_model.evaluate(mnist_test_x, mnist_test_y)\n",
        "print('FFNN with 1 hidden layer; Accuracy: {}'.format(acc))\n",
        "\n",
        "mnist_ffnn_2_model.load_weights('mnist_ffnn_2_best.h5')\n",
        "loss, acc = mnist_ffnn_2_model.evaluate(mnist_test_x, mnist_test_y)\n",
        "print('FFNN with 2 hidden layers; Accuracy: {}'.format(acc))\n",
        "\n",
        "mnist_ffnn_2_l2_model.load_weights('mnist_ffnn_2_best.h5')\n",
        "loss, acc = mnist_ffnn_2_l2_model.evaluate(mnist_test_x, mnist_test_y)\n",
        "print('FFNN with 2 hidden layers and L2 regularization; Accuracy: {}'.format(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU1Z1DDXSWq7",
        "colab_type": "text"
      },
      "source": [
        "## Dropout Regularization on CNN\n",
        "Taking CNN with two hidden layers and applying Dropout:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-OTwW70Cyg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_cnn_2_drop_model = tf.keras.Sequential(name='mnist_cnn_2_drop')\n",
        "mnist_cnn_2_drop_model.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n",
        "mnist_cnn_2_drop_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=4, activation='relu', padding='same', name='convolution1'))\n",
        "mnist_cnn_2_drop_model.add(tf.keras.layers.MaxPool2D(pool_size=2, name='pooling1'))\n",
        "mnist_cnn_2_drop_model.add(tf.keras.layers.Conv2D(filters=25, kernel_size=3, activation='relu', padding='same', name='convolution2'))\n",
        "mnist_cnn_2_drop_model.add(tf.keras.layers.MaxPool2D(pool_size=3, name='pooling2'))\n",
        "mnist_cnn_2_drop_model.add(tf.keras.layers.Dropout(0.5, name='dropout'))\n",
        "mnist_cnn_2_drop_model.add(tf.keras.layers.Flatten(name='flatten'))\n",
        "mnist_cnn_2_drop_model.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n",
        "mnist_cnn_2_drop_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "mnist_cnn_2_drop_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA7NT93m6oCP",
        "colab_type": "text"
      },
      "source": [
        "Let's train the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl0td5O3EO9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_cnn_2_drop_best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n",
        "\n",
        "mnist_cnn_2_drop_model_train = mnist_cnn_2_drop_model.fit(mnist_train_x, mnist_train_y, validation_split=0.2, callbacks=[earlystop,checkpoint], epochs=10000, batch_size=256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCe4uVs2AkIW",
        "colab_type": "text"
      },
      "source": [
        "By looking at the evolution, we can see that the performance of the model on the training data is now lower."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doyFaOZdEYCS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n",
        "\n",
        "loss_ax.set_title('Loss')\n",
        "loss_ax.plot(mnist_cnn_2_drop_model_train.history['loss'], '-r', label='Train')\n",
        "loss_ax.plot(mnist_cnn_2_drop_model_train.history['val_loss'], '-g', label='Validation')\n",
        "\n",
        "acc_ax.set_title('Accuracy')\n",
        "acc_ax.plot(mnist_cnn_2_drop_model_train.history['accuracy'], '-r', label='Train')\n",
        "acc_ax.plot(mnist_cnn_2_drop_model_train.history['val_accuracy'], '-g', label='Validation')\n",
        "\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtc9e509BLv0",
        "colab_type": "text"
      },
      "source": [
        "And we can assess the performance on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skwDZOoSEYkY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_baseline_model.load_weights('mnist_baseline_best.h5')\n",
        "loss, acc = mnist_baseline_model.evaluate(mnist_test_x, mnist_test_y)\n",
        "print('Accuracy: {}'.format(acc))\n",
        "\n",
        "mnist_conv_model.load_weights('mnist_conv_best.h5')\n",
        "loss, acc = mnist_conv_model.evaluate(mnist_test_x, mnist_test_y)\n",
        "print('Accuracy: {}'.format(acc))\n",
        "\n",
        "mnist_cnn_2_model.load_weights('mnist_cnn_2_best.h5')\n",
        "loss, acc = mnist_cnn_2_model.evaluate(mnist_test_x, mnist_test_y)\n",
        "print('Accuracy: {}'.format(acc))\n",
        "\n",
        "mnist_cnn_2_drop_model.load_weights('mnist_cnn_2_drop_best.h5')\n",
        "loss, acc = mnist_cnn_2_drop_model.evaluate(mnist_test_x, mnist_test_y)\n",
        "print('Accuracy: {}'.format(acc))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}